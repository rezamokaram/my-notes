# 🔁 What Happens to Token Range Data If a Node Goes Down in ScyllaDB?

When a node in ScyllaDB goes down, the system is designed to ensure **continued availability** of the data, including the token range that node was responsible for.

---

## 🧠 Replication in ScyllaDB

- ScyllaDB uses a **Replication Factor (RF)** to store **multiple copies** of each partition.
- Example: With `RF = 3`, each piece of data is stored on **3 different nodes**.

---

## 📦 Token Ownership Example

Let’s say:

- **Node A** owns token range `1000–2000`.
- **Node B** and **Node C** are replicas for that same range.
- Data with token `1500` is stored on:
  - Node A (primary replica)
  - Node B (replica)
  - Node C (replica)

---

## 🚨 Node A Goes Down

Even though Node A is the **primary owner** of token range `1000–2000`, the data is still:

- Available on **Node B** and **Node C**.
- Accessible by the **coordinator node**, which knows where all replicas live.

---

## 🔄 Coordinator Node Behavior

- Any node that receives a request becomes the **coordinator**.
- It uses metadata to determine:
  - Which token range the data belongs to.
  - Which nodes have replicas.
  - Which nodes are alive.
- The coordinator **routes the request** to available replica nodes.

---

## ✅ Example: Read Operation

- **Replication Factor** = 3
- **Node A** is down
- **Node B** and **Node C** are up
- Client reads partition with token `1500`
- **Consistency Level** = `QUORUM` (2 out of 3)
- Coordinator contacts Node B and Node C → **read succeeds**

---

## ✍️ Example: Write Operation

- Same setup
- Client writes data for token `1500`
- Node A is down
- Write goes to Node B and Node C
- If `Consistency Level = QUORUM` → **write succeeds**
- A **hint** is stored for Node A to replay later (hinted handoff)

---

## 📊 Visualization

```txt
Token Range 1000–2000:

Node A (Down)     → Replica 1 (Primary owner)
Node B (Alive)    → Replica 2
Node C (Alive)    → Replica 3

Read/Write to token 1500:
→ Coordinator contacts Node B and Node C
→ Operation succeeds
```

---

## 🔐 Why This Works

| Feature                  | Role in Availability                                |
|--------------------------|-----------------------------------------------------|
| Replication Factor       | Ensures multiple copies of each partition           |
| Coordinator Node         | Knows which nodes have the data                     |
| Token Ring Awareness     | Nodes understand full cluster topology              |
| Tunable Consistency      | Allows tradeoff between speed and fault tolerance   |
| Hinted Handoff & Repair  | Ensures failed node can catch up on missed writes   |

---

## **How the Coordinator Finds the Replicas:**

1. **Read or Write Request**:
   - When a client sends a read/write request for data in the range `1000–2000`, the coordinator (any node in the cluster) will:
     - Hash the **partition key** to find the token.
     - Identify which node should have the primary replica for the token (Node B in this case).
     - Check the **replica nodes** (Node A and Node C) and send the request to **the available nodes**.
  
2. **Consistency Level (e.g., QUORUM)**:
   - If the consistency level is **QUORUM** (majority of replicas), the coordinator will wait for responses from at least **2 out of 3 replicas** (Node A and Node C in this case).
   - If Node B is down, the request will still succeed using the available replicas.

### **Hinted Handoff & Recovery:**

- **Hinted Handoff**:
  - If a node is down during a write, ScyllaDB will store a **"hint"** about the write operation.
  - Once the node comes back online, it will receive the hints and apply any missed writes.

- **Repair Operations**:
  - ScyllaDB periodically performs **repair operations** to ensure that replicas are in sync.
  - If data is missing due to a node being down, a repair will synchronize the replicas.

## Key Concept: Static Replication for RF=3

For RF=3, each partition has exactly 3 fixed replicas, one on the primary node (responsible for the token range) and two others placed in a predefined and fixed order based on the consistent hashing ring.
Example:

```txt
Let’s say you have 10 nodes in a cluster with RF = 3:

    Node 1 owns token range 0 - 100, and the replicas for this range are on:

        Node 2

        Node 3

    Node 2 owns token range 101 - 200, and the replicas for this range are on:

        Node 3

        Node 4

    And so on, where each node knows the replicas for its own token range and the replicas of other token ranges.
```

## 🔄 Do Replication Nodes Change?

No, replication nodes do not change in the normal operation of the cluster, but there are some exceptions:

- Node Addition/Removal: When you add or remove nodes from the cluster, ScyllaDB will rebalance the data to ensure all token ranges are distributed across nodes, and replication is maintained. This might result in some data being re-replicated to different nodes.
- Token Range Rebalancing: If you reconfigure the cluster or the virtual nodes (vnodes) change, data may be re-replicated across different nodes, but that’s part of cluster maintenance.
- Replication Strategy: If you switch between SimpleStrategy and NetworkTopologyStrategy, or change your replication factor, this will affect how and where data is replicated.

## ScyllaDB: Metadata and Replica Node Locations

## **Does Metadata Contain the Location of Replica Nodes?**

Yes, **metadata in ScyllaDB** contains information about the **location of replica nodes**, along with other critical data like token ranges, cluster topology, and node status. This metadata is essential for routing requests and ensuring **data availability** even if a node goes down.

---

## **Key Components of Metadata for Replication**

1. **Token Range Mapping**:
   - Each node in ScyllaDB knows which **token range** it is responsible for.
   - The **gossip protocol** ensures that all nodes are aware of the **entire token ring** and the ranges owned by other nodes.

2. **Replica Information**:
   - Based on the **replication factor (RF)** and the **replication strategy** (e.g., **SimpleStrategy** or **NetworkTopologyStrategy**), the metadata stores information about which nodes are the **replicas** for each data partition.
   - For example, if **RF = 3**, each data partition will have **3 replicas** spread across 3 nodes.

3. **Cluster Membership**:
   - ScyllaDB keeps track of all nodes in the cluster in the **system tables** (`system.peers`, `system.local`).
   - These tables store metadata about:
     - **IP addresses** of all nodes
     - **Token ranges** each node is responsible for
     - **Node status** (whether nodes are up or down)

## **Where is the Metadata Stored?**

1. **System Tables**:
   - **`system.peers`**: Contains information about other nodes in the cluster.
   - **`system.local`**: Contains information about the local node’s token range and other details.
   - These tables store metadata about **replicas** and **cluster topology**.

2. **Gossip Protocol**:
   - The **gossip protocol** ensures that metadata about node status, token ranges, and replica locations is constantly shared and updated between all nodes in the cluster.

## ✅ Summary

> **Even if a node responsible for a token range fails, ScyllaDB can still serve reads and writes for that range using other replicas.**

Thanks to replication and intelligent request routing, there is **no downtime** or **data unavailability** — assuming proper configuration (e.g. `RF >= 3`).

- Replication Nodes for RF = 3 are static in the sense that each piece of data is replicated to 3 fixed nodes based on the consistent hashing of the token range.

- Replicas don’t change dynamically unless the cluster topology changes (node additions/removals, rebalancing, etc.).

- The consistency of replicas is maintained even when a node goes down, and the system reroutes requests to available replicas.
